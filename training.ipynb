{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SingleFileDetectionDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.df = pd.read_parquet(file_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        # Define the class mapping\n",
    "        self.class_mapping = {\n",
    "            0: 0,  # background\n",
    "            1: 1,  # person\n",
    "            2: 2,  # bicycle\n",
    "            3: 3,  # car\n",
    "            4: 4,  # motorcycle\n",
    "            75: 5, # clock\n",
    "            68: 6, # cell phone\n",
    "            10: 7, # traffic light\n",
    "            12: 8, # stop sign\n",
    "            6: 9,  # bus\n",
    "            44: 10 # knife\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Decode the PNG image data\n",
    "        image = Image.open(io.BytesIO(row['image']))\n",
    "        \n",
    "        # Convert to RGB if it's not already\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Apply the transform to convert to tensor\n",
    "        image = self.transform(image)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in row['annotations']:\n",
    "            # Extract bounding box coordinates\n",
    "            x, y, w, h = ann['bbox']\n",
    "            \n",
    "            # Convert to (x1, y1, x2, y2) format and ensure positive width and height\n",
    "            x1 = round(x)\n",
    "            y1 = round(y)\n",
    "            x2 = round(x + max(1, w))  # Ensure width is at least 1\n",
    "            y2 = round(y + max(1, h))  # Ensure height is at least 1\n",
    "            \n",
    "            # Append the corrected bounding box\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            \n",
    "            # Remap the class ID\n",
    "            original_class_id = ann['category_id']\n",
    "            remapped_class_id = self.class_mapping.get(original_class_id, 0)  # Default to background if not found\n",
    "            labels.append(remapped_class_id)\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "# The rest of the code remains the same\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def load_model(num_classes):\n",
    "    model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=True,trainable_backbone_layers=6)\n",
    "    \n",
    "    # Replace the classifier with a new one for our number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    # Calculate IoU between two bounding boxes\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    total_iou = 0\n",
    "    num_boxes = 0\n",
    "    \n",
    "    for images, targets in tqdm(dataloader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Calculate IoU\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "            for pred, target in zip(predictions, targets):\n",
    "                pred_boxes = pred['boxes']\n",
    "                true_boxes = target['boxes']\n",
    "                for pred_box in pred_boxes:\n",
    "                    ious = [calculate_iou(pred_box.cpu(), true_box.cpu()) for true_box in true_boxes]\n",
    "                    if ious:\n",
    "                        total_iou += max(ious)\n",
    "                        num_boxes += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_iou = total_iou / num_boxes if num_boxes > 0 else 0\n",
    "    return avg_loss, avg_iou\n",
    "\n",
    "def main():\n",
    "    num_classes = 11  # Number of classes in filtered_classes\n",
    "    num_epochs = 40  # Increase total number of epochs\n",
    "    batch_size = 4\n",
    "    learning_rate = 0.0001\n",
    "    start_epoch = 1  # Start from the 10th epoch\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(num_classes)\n",
    "    \n",
    "    # Load the saved state dict from the 10th epoch\n",
    "    checkpoint_path = f'fasterrcnn_mobilenet_v3_epoch_{start_epoch}.pth'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "        print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Starting from scratch.\")\n",
    "        start_epoch = 0\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params, lr=learning_rate)\n",
    "    \n",
    "    # Get list of all parquet files\n",
    "    all_files = glob.glob('./filter 640/filtered_dataset_train_640_*.parquet')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_loss = 0\n",
    "        epoch_iou = 0\n",
    "        \n",
    "        for file_path in all_files:\n",
    "            print(f\"Training on file: {file_path}\")\n",
    "            # Load dataset for current file\n",
    "            train_dataset = SingleFileDetectionDataset(file_path)\n",
    "            train_dataloader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=True, \n",
    "                num_workers=2, \n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "            \n",
    "            # Train on current file\n",
    "            loss, iou = train_one_epoch(model, train_dataloader, optimizer, device)\n",
    "            epoch_loss += loss\n",
    "            epoch_iou += iou\n",
    "            print(f\"File Loss: {loss:.4f}, File IoU: {iou:.4f}\")\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(all_files)\n",
    "        avg_epoch_iou = epoch_iou / len(all_files)\n",
    "        print(f\"Epoch Average Loss: {avg_epoch_loss:.4f}, Epoch Average IoU: {avg_epoch_iou:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), f'fasterrcnn_mobilenet_v3_epoch_{epoch+1}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhammadfasi/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/muhammadfasi/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1591/1591 [00:33<00:00, 47.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.2244\n",
      "mAP (small): 0.0910\n",
      "mAP (medium): 0.3568\n",
      "mAP (large): 0.4584\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15853, 34083]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 116\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[5], line 102\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m     y_scores\u001b[38;5;241m.\u001b[39mextend(pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m][pred_masks]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_true) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_scores) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     precisions, recalls, thresholds \u001b[38;5;241m=\u001b[39m precision_recall_curve(y_true, y_scores)\n\u001b[1;32m    103\u001b[0m     ap \u001b[38;5;241m=\u001b[39m average_precision_score(y_true, y_scores)\n\u001b[1;32m    104\u001b[0m     plot_precision_recall_curve(precisions, recalls, ap, class_id)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:878\u001b[0m, in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_recall_curve\u001b[39m(y_true, probas_pred, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision-recall pairs for different probability thresholds.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;124;03m    array([0.1 , 0.35, 0.4 , 0.8 ])\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    879\u001b[0m         y_true, probas_pred, pos_label\u001b[38;5;241m=\u001b[39mpos_label, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[1;32m    880\u001b[0m     )\n\u001b[1;32m    882\u001b[0m     ps \u001b[38;5;241m=\u001b[39m tps \u001b[38;5;241m+\u001b[39m fps\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# Initialize the result array with zeros to make sure that precision[ps == 0]\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# does not contain uninitialized values.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:751\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m--> 751\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    752\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[1;32m    753\u001b[0m y_score \u001b[38;5;241m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [15853, 34083]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ... [Keep your existing SingleFileDetectionDataset, collate_fn, and load_model functions] ...\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    print(len(dataloader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(dataloader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i in range(len(outputs)):\n",
    "                all_preds.append({\n",
    "                    'boxes': outputs[i]['boxes'].cpu(),\n",
    "                    'scores': outputs[i]['scores'].cpu(),\n",
    "                    'labels': outputs[i]['labels'].cpu()\n",
    "                })\n",
    "                all_targets.append({\n",
    "                    'boxes': targets[i]['boxes'].cpu(),\n",
    "                    'labels': targets[i]['labels'].cpu()\n",
    "                })\n",
    "    \n",
    "    metric.update(all_preds, all_targets)\n",
    "    results = metric.compute()\n",
    "    \n",
    "    return results, all_preds, all_targets\n",
    "\n",
    "def plot_precision_recall_curve(precisions, recalls, ap, class_id):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recalls, precisions, label=f'Precision-Recall curve (AP = {ap:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve for Class {class_id}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'precision_recall_curve_class_{class_id}.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    num_classes = 11\n",
    "    batch_size = 2\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_model(num_classes)\n",
    "    model.load_state_dict(torch.load('./run 2/fasterrcnn_mobilenet_v3_epoch_38.pth'))  # Load the last epoch's weights\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_dataset = SingleFileDetectionDataset('/home/muhammadfasi/Downloads/FYP/scripts/testing/filtered_test_256.parquet')\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results, all_preds, all_targets = evaluate(model, test_dataloader, device)\n",
    "    # Print mAP results\n",
    "    print(f\"mAP: {results['map']:.4f}\")\n",
    "    print(f\"mAP (small): {results['map_small']:.4f}\")\n",
    "    print(f\"mAP (medium): {results['map_medium']:.4f}\")\n",
    "    print(f\"mAP (large): {results['map_large']:.4f}\")\n",
    "    \n",
    "    # Calculate and plot precision-recall curve for each class\n",
    "    for class_id in range(1, num_classes):  # Exclude background class\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        \n",
    "        for pred, target in zip(all_preds, all_targets):\n",
    "            pred_masks = pred['labels'] == class_id\n",
    "            target_masks = target['labels'] == class_id\n",
    "            \n",
    "            y_true.extend(target_masks.tolist())\n",
    "            y_scores.extend(pred['scores'][pred_masks].tolist())\n",
    "        \n",
    "        if len(y_true) > 0 and len(y_scores) > 0:\n",
    "            precisions, recalls, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "            ap = average_precision_score(y_true, y_scores)\n",
    "            plot_precision_recall_curve(precisions, recalls, ap, class_id)\n",
    "    \n",
    "    # Plot mAP\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Overall', 'Small', 'Medium', 'Large'], \n",
    "            [results['map'], results['map_small'], results['map_medium'], results['map_large']])\n",
    "    plt.title('Mean Average Precision (mAP)')\n",
    "    plt.ylabel('mAP')\n",
    "    plt.savefig('map_results.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
