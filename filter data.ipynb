{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../datasets/640x640/resized_dataset_train_640_1.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_1.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_2.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_2.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_3.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_3.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_4.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_4.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_5.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_5.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_6.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_6.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_7.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_7.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_8.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_8.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_9.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_9.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_10.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_10.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_11.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_11.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_12.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_12.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_13.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_13.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_14.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_14.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_15.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_15.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_16.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_16.parquet\n",
      "Processing ../datasets/640x640/resized_dataset_train_640_17.parquet\n",
      "Processed and saved filter 640/filtered_dataset_train_640_17.parquet\n",
      "Filtering complete.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "\n",
    "filtered_classes = {\n",
    "    1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle',\n",
    "    75: 'clock', 68: 'cell phone', 10: 'traffic light',\n",
    "    12: 'stop sign', 0: '__background__', 6: 'bus', 44: 'knife'\n",
    "}\n",
    "\n",
    "def filter_annotations(annotations):\n",
    "    return [ann for ann in annotations if ann['category_id'] in filtered_classes]\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    chunk = chunk.to_pandas()\n",
    "    chunk['annotations'] = chunk['annotations'].apply(filter_annotations)\n",
    "    return chunk[chunk['annotations'].apply(len) > 0]\n",
    "\n",
    "def process_file(file_name, output_file):\n",
    "    parquet_file = pq.ParquetFile(file_name)\n",
    "    \n",
    "    writer = None\n",
    "    for batch in parquet_file.iter_batches(batch_size=10000):\n",
    "        filtered_chunk = process_chunk(batch)\n",
    "        if not filtered_chunk.empty:\n",
    "            if writer is None:\n",
    "                schema = pa.Schema.from_pandas(filtered_chunk)\n",
    "                writer = pq.ParquetWriter(output_file, schema)\n",
    "            table = pa.Table.from_pandas(filtered_chunk)\n",
    "            writer.write_table(table)\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "def main():\n",
    "    output_dir = \"filter 640\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(1, 18):\n",
    "        input_file = f\"../datasets/640x640/resized_dataset_train_640_{i}.parquet\"\n",
    "        output_file = os.path.join(output_dir, f\"filtered_dataset_train_640_{i}.parquet\")\n",
    "        \n",
    "        print(f\"Processing {input_file}\")\n",
    "        process_file(input_file, output_file)\n",
    "        print(f\"Processed and saved {output_file}\")\n",
    "\n",
    "    print(\"Filtering complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_17.parquet: 1550 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_16.parquet: 1606 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_24.parquet: 703 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_21.parquet: 1630 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_15.parquet: 1553 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_18.parquet: 1605 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_10.parquet: 1579 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_19.parquet: 1512 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_7.parquet: 1571 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_11.parquet: 1700 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_6.parquet: 1668 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_1.parquet: 1512 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_20.parquet: 1545 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_3.parquet: 1572 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_4.parquet: 1571 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_5.parquet: 1616 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_14.parquet: 1573 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_23.parquet: 1636 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_22.parquet: 1620 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_2.parquet: 1634 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_9.parquet: 1518 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_8.parquet: 1525 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_12.parquet: 1628 rows\n",
      "/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_13.parquet: 1649 rows\n",
      "\n",
      "Total rows across all files: 37276\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path pattern for your parquet files\n",
    "file_pattern = '/home/muhammadfasi/Downloads/FYP/scripts/filter 640/filtered_dataset_train_640_*.parquet'\n",
    "\n",
    "# Get all matching file names\n",
    "file_names = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize a counter for total rows\n",
    "total_rows = 0\n",
    "\n",
    "# Loop through each file\n",
    "for file_name in file_names:\n",
    "    # Read the parquet file\n",
    "    df = pd.read_parquet(file_name)\n",
    "    \n",
    "    # Add the number of rows to the total\n",
    "    total_rows += len(df)\n",
    "    \n",
    "    # Print the count for each file\n",
    "    print(f\"{file_name}: {len(df)} rows\")\n",
    "\n",
    "# Print the total count\n",
    "print(f\"\\nTotal rows across all files: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
